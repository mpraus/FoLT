{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S5W5BuAMLpTE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dropout, Dense\n",
    "from keras import callbacks\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iYuH3GjRLpTH"
   },
   "outputs": [],
   "source": [
    "def data_reader(filename:str):\n",
    "    \"\"\"Read the movie reviews with the given filename.\"\"\"\n",
    "    reviews = []\n",
    "    rating = np.array(0)\n",
    "    def parse(x):\n",
    "        if x == 'positive':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        df = pd.read_csv(filename)\n",
    "        df['sentiment'] = df['sentiment'].apply(lambda x: parse(x))\n",
    "    return (df['review'].tolist(), df['sentiment'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if method is working\n",
    "# print(data_reader('IMDB Dataset.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZOoHO3beLpTJ"
   },
   "outputs": [],
   "source": [
    "def load_fast_text_embeddings(filename: str):\n",
    "    \"\"\"Loads the FastText embeddings from the file with the given filename.\"\"\"\n",
    "    \n",
    "    dictionary = {}\n",
    "    with open(filename) as reader:\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            line = line.strip()\n",
    "            dictionary[line.split(' ')[0]] = np.array([float(x) for x in line.split(' ')[1:]])\n",
    "    return (dictionary, len(dictionary[list(dictionary.keys())[0]]))\n",
    "\n",
    "token2vector, dimensions =load_fast_text_embeddings(filename=embedding_path)\n",
    "assert(dimensions == 300)\n",
    "\n",
    "assert(token2vector['article'][0] == -0.1303 )\n",
    "\n",
    "assert(token2vector['state'][2] ==0.098 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vyjp_GMPLpTJ"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences: [str]):\n",
    "    \"\"\"Tokenizes the given sentences\"\"\"\n",
    "    sents = []\n",
    "    for sentence in sentences:\n",
    "        tokens = wordpunct_tokenize(sentence)\n",
    "        sents.append([word.lower() for word in tokens if not word in nltk.corpus.stopwords.words('english') and word.isalpha()])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "gUMS5G9uLpTK"
   },
   "outputs": [],
   "source": [
    "def average(sentence):\n",
    "    return np.array(np.array([token2vector.get(token, np.zeros(300)) for token in sentence])).mean(axis=0)\n",
    "\n",
    "def map_to_vectors(tokenized_sentences: [[str]]):\n",
    "    \"\"\"Maps the given tokenized sentences to lists of vectors.\"\"\"\n",
    "    return np.array([average(sent) for sent in tokenized_sentences])\n",
    "\n",
    "embedded_reviews = map_to_vectors(tokenized_reviews)\n",
    "\n",
    "assert(len(embedded_reviews[0]) == 300)\n",
    "\n",
    "assert(len(embedded_reviews[100]) == 300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLGMwfLYLpTK"
   },
   "outputs": [],
   "source": [
    "# define a model\n",
    "def MLP(input_shape: np.array):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100,activation='relu',input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TLOpeOlJVUzX"
   },
   "outputs": [],
   "source": [
    "mode = None\n",
    "#______________________________________________\n",
    "# uncomment by tutors\n",
    "#mode = \"grading\"\n",
    "#______________________________________________\n",
    "if mode == \"grading\":\n",
    "    data_path = '../dataset and embeddings/IMDB Dataset.csv'\n",
    "    embedding_path  = '../dataset and embeddings/wiki-news-300d-10k.vec'\n",
    "    model_path = '../model/best_model.ckpt'\n",
    "else:\n",
    "    # TODO\n",
    "    data_path = 'IMDB Dataset.csv'\n",
    "    embedding_path = 'embeddings/wiki-news-300d-10k.vec'\n",
    "    model_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TWkyYvZ2xHS9"
   },
   "outputs": [],
   "source": [
    "reviews, sentiments = data_reader(data_path)\n",
    "# convert numeric labels to one hot vectors\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "sentiments = sentiments.reshape(len(sentiments), 1)\n",
    "sentiments = onehot_encoder.fit_transform(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "klHLNuOlxKTH"
   },
   "outputs": [],
   "source": [
    "token2vector, dimensions = load_fast_text_embeddings(filename=embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "p3i282FhxMVU"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cdcdfaa0eee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d01d512b5422>\u001b[0m in \u001b[0;36mtokenize_sentences\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d01d512b5422>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     21\u001b[0m         return [\n\u001b[1;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ]\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, encoding, errors)\u001b[0m\n\u001b[1;32m   1036\u001b[0m            beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \"\"\"The length of the byte order marker at the beginning of\n\u001b[1;32m   1040\u001b[0m            the stream (or None for no byte order marker).\"\"\"\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_check_bom\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;31m# Normalize our encoding name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[ -]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;31m# Look up our encoding in the BOM table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_reviews = tokenize_sentences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCMPDyIUxOky"
   },
   "outputs": [],
   "source": [
    "embedded_reviews = map_to_vectors(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve03wucrW-7H"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# split the vectorized reviews into train-, and testset\n",
    "train_test_split = 0.8\n",
    "train_x = embedded_reviews[:len(embedded_reviews)*train_test_split]\n",
    "train_y = sentiments[:len(embedded_reviews)*train_test_split]\n",
    "test_x = embedded_reviews[len(embedded_reviews)*train_test_split:]\n",
    "test_y = sentiments[len(embedded_reviews)*train_test_split:]\n",
    "#ENDTODO\n",
    "\n",
    "# train the model and save the best one on dev set\n",
    "cp_callback = callbacks.ModelCheckpoint(filepath=model_path, verbose=1, save_weights_only=True, monitor='val_acc', save_best_only=True)\n",
    "model = MLP(input_shape=(dimensions,))\n",
    "# use the binary cross entropy to measure the errors\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs=4, batch_size=30, callbacks=[cp_callback], validation_split=0.2)\n",
    "\n",
    "model.load_weights(model_path)\n",
    "# calculate the accuracy on test set\n",
    "_,acc = model.evaluate(test_x,test_y)\n",
    "print('accuracy on test set:', acc)\n",
    "\n",
    "predictions=model.predict(test_x)\n",
    "# convert predictions to one hot vectors\n",
    "predictions_oneHot = np.where(predictions > 0.5, 1, 0)\n",
    "#print(predictions_oneHot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-4gUp0iqwIb"
   },
   "outputs": [],
   "source": [
    "print('First Review')\n",
    "print(tokenized_reviews[0])\n",
    "print('Second Review')\n",
    "print(tokenized_reviews[5])\n",
    "print('Third Review')\n",
    "print(tokenized_reviews[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3bAwMv6Op11"
   },
   "source": [
    "(1 Point) point for implementing tokenize_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHDz877BOf-_"
   },
   "outputs": [],
   "source": [
    "#______________________________________________\n",
    "# should be written by tutors\n",
    "point_for_c = \n",
    "#______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRz0ofSNLpTM"
   },
   "source": [
    "(e)(4 Points) Check the functionMLP(input_shape:np.array)that defines our multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMuBC2zELpTM"
   },
   "source": [
    "(i)(1 Point) How many layers does this model have (including the input and the output layer)?\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDa_fCeZLpTN"
   },
   "outputs": [],
   "source": [
    "#______________________________________________\n",
    "# should be written by tutors\n",
    "point_for_e_i = \n",
    "#______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8kLx38DLpTO"
   },
   "source": [
    "(ii) (1 Point) What is the size of the matrix that connects the input layer and the first hidden layer? \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0cElAF2LpTO"
   },
   "outputs": [],
   "source": [
    "#______________________________________________\n",
    "# should be written by tutors\n",
    "point_for_e_ii = \n",
    "#______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yZejlDkLpTP"
   },
   "source": [
    "(iii) (1 Point) How many units/neurons are in the output layer? Why? \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, because our categories are binary (negative, positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLJS8QvgLpTP"
   },
   "outputs": [],
   "source": [
    "#______________________________________________\n",
    "# should be written by tutors\n",
    "point_for_e_iii =\n",
    "#______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_rYzkXQLpTP"
   },
   "source": [
    "(iv) (1 Point)  What is the meaning of Dropout? \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout means that some of the input is randomly set to 0, in this case with a frequency of 0.2. This helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrRN0jM9LpTQ"
   },
   "outputs": [],
   "source": [
    "#______________________________________________\n",
    "# should be written by tutors\n",
    "point_for_e_iv = \n",
    "#______________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGvRVMzCLpTS"
   },
   "outputs": [],
   "source": [
    "# the following codes are used for grading, students can simply ignore them, but please don't change/delete them\n",
    "# any modifications to the code are seen to be cheating\n",
    "import csv\n",
    "assertions = dict()\n",
    "with open('../assertions.csv',newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        assertions[row['assertion sequence']] = row['content']\n",
    "\n",
    "total_points = 0\n",
    "\n",
    "\n",
    "try:  \n",
    "    assert(assertions['a_1'] in reviews)\n",
    "    assert(sentiments[reviews.index(assertions['a_1'])][1]==1)\n",
    "    total_points += 1\n",
    "except: \n",
    "    print('errors in your implementation for question (a)')\n",
    "\n",
    "try:\n",
    "    assert(total_points <= 1)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question (a)')\n",
    "\n",
    "\n",
    "try:  \n",
    "    assert(dimensions == int(assertions['b_1']))\n",
    "    assert(token2vector['article'][0] == float(assertions['b_2']))\n",
    "    assert(token2vector['state'][2] == float(assertions['b_3']))\n",
    "    total_points += 1\n",
    "except AssertionError:\n",
    "    print('errors in your implementation for question (b)')\n",
    "\n",
    "\n",
    "try:\n",
    "    assert(total_points <= 2)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question (b)')\n",
    "    \n",
    "\n",
    "total_points += point_for_c \n",
    "try:\n",
    "    assert(total_points <= 3)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question (c)')\n",
    "    \n",
    "\n",
    "try:\n",
    "    assert(len(embedded_reviews[0]) == int(assertions['d_1']))\n",
    "    assert(len(embedded_reviews[100]) == int(assertions['d_2']))\n",
    "    assert(len(embedded_reviews[600]) == int(assertions['d_3']))\n",
    "    total_points += 1\n",
    "except AssertionError:\n",
    "    print('errors in your implementation for question (d)')\n",
    "\n",
    "try:\n",
    "    assert(total_points <= 4)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question (d)')\n",
    "\n",
    "points_for_first_four_questions = total_points\n",
    "\n",
    "\n",
    "\n",
    "total_points += point_for_e_i \n",
    "try:\n",
    "    assert(total_points <= 5)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question e(i)')\n",
    "\n",
    "\n",
    "total_points += point_for_e_ii\n",
    "try:\n",
    "    assert(total_points <= 6)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question e(ii)')\n",
    "\n",
    "\n",
    "total_points += point_for_e_iii \n",
    "try:\n",
    "    assert(total_points <= 7)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question e(iii)')\n",
    "\n",
    " \n",
    "total_points += point_for_e_iv \n",
    "try:\n",
    "    assert(total_points <= 8)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question e(iv)')\n",
    "\n",
    "\n",
    "point_for_f = 0\n",
    "try:\n",
    "    assert(len(train_x) > len(test_x))\n",
    "    assert(len(train_y) > len(test_y))\n",
    "    assert(len(train_x)+len(test_x)==len(embedded_reviews))\n",
    "    assert(len(train_y)+len(test_y)==len(sentiments))\n",
    "    point_for_f = 2\n",
    "    total_points += point_for_f\n",
    "except AssertionError:\n",
    "    print('errors in your implementation for question (f)')\n",
    "\n",
    "try:\n",
    "    assert(total_points <= 10)\n",
    "except AssertionError:\n",
    "    print('errors in calculating the points for question (f)')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-STN17sLpTU"
   },
   "outputs": [],
   "source": [
    "print(\"points for the first four questions: \",points_for_first_four_questions)\n",
    "print(\"point for e(i): \", point_for_e_i)\n",
    "print(\"point for e(ii): \", point_for_e_ii)\n",
    "print(\"point for e(iii): \", point_for_e_iii)\n",
    "print(\"point for e(iv): \", point_for_e_iv)\n",
    "print(\"point for f: \", point_for_f)\n",
    "print(\"total points: \", total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbvNsGfpLpTV"
   },
   "outputs": [],
   "source": [
    "# the following codes can be run if you want to save the result of all grading to a .csv file\n",
    "# it can be run on the window system, but if you use colab, you need to at first locate to your current .ipynb file (cd xxx/xxx)\n",
    "import os\n",
    "# for windows users\n",
    "filename = os.getcwd().split('\\\\')[-1]\n",
    "#filename = os.getcwd().split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_qgCisELpTW"
   },
   "outputs": [],
   "source": [
    "name = filename.split('_')[0]\n",
    "matrikelnr= filename.split('_')[1]\n",
    "# do you have any feedback\n",
    "feedback = \"\"\n",
    "#feedback += ' '.join([item for item in comments if len(item.strip())>0])\n",
    "with open('../grading_for_HW9.csv','a+',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([name,matrikelnr,total_points,feedback])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Edited_HW9_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
