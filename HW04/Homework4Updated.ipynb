{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QclFG5At3yv9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk==3.5 in /home/alexander/.local/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: click in /usr/lib/python3.8/site-packages (from nltk==3.5) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/alexander/.local/lib/python3.8/site-packages (from nltk==3.5) (0.17.0)\n",
      "Requirement already satisfied: regex in /home/alexander/.local/lib/python3.8/site-packages (from nltk==3.5) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/alexander/.local/lib/python3.8/site-packages (from nltk==3.5) (4.51.0)\n",
      "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package udhr to /home/alexander/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell once before doing anything else\n",
    "!pip install --target=$nb_path nltk==3.5\n",
    "!python3 -m nltk.downloader udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4ZjpTzGSGIGq"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eJbvgQfHXvb"
   },
   "source": [
    "## Homework 4.1 (6 points)\n",
    "### Alexander Praus, Maike Arnold\n",
    "\n",
    "Implement a language guesser, i.e. a function that takes a given text and outputs the language it thinks the text is\n",
    "written in. The function should base its decision on the frequency of individual characters in each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "ucDND7klGVMg"
   },
   "outputs": [],
   "source": [
    "# build the language models\n",
    "# udhr contains the Universal Declaration of Human Rights in over 300 languages\n",
    "languages = ['English', 'German_Deutsch', 'Spanish']\n",
    "language_base = dict((language, udhr.words(language + '-Latin1')) for language in languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzUAUu8d6xr0"
   },
   "source": [
    "a) Implement a function `build_language_models(languages,words)` which takes a list of languages and a\n",
    "dictionary of words as arguments and returns a conditional frequency distribution where:\n",
    "*   the languages are the conditions\n",
    "*   the values are the lower case characters found in `words[language]`\n",
    "\n",
    "Call the function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "W0LXcNdp4AWJ"
   },
   "outputs": [],
   "source": [
    "def build_language_models(languages, words):\n",
    "    freqDist = nltk.ConditionalFreqDist((language, char.lower()) for language in languages for word in words[language] for char in word)  \n",
    "    return freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "_n4FJ9nMJGCP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'German_Deutsch', 'Spanish']\n",
      "English u -> 0.02212549873050417\n",
      "English n -> 0.08076411558457261\n",
      "English i -> 0.07882964575021158\n",
      "English v -> 0.011606819006166122\n",
      "English e -> 0.12017893845967839\n",
      "English r -> 0.06770644420263572\n",
      "English s -> 0.05126345061056704\n",
      "English a -> 0.08040140249062991\n",
      "English l -> 0.04570184983677911\n",
      "English d -> 0.03615040502962157\n",
      "German_Deutsch d -> 0.054984823721690404\n",
      "German_Deutsch i -> 0.0745972449217838\n",
      "German_Deutsch e -> 0.16845668923651647\n",
      "German_Deutsch a -> 0.053116974083586274\n",
      "German_Deutsch l -> 0.04027550782162036\n",
      "German_Deutsch g -> 0.03829091758113472\n",
      "German_Deutsch m -> 0.016577165538174177\n",
      "German_Deutsch n -> 0.10156432407191221\n",
      "German_Deutsch r -> 0.07623161335512492\n",
      "German_Deutsch k -> 0.011440579033387813\n",
      "Spanish d -> 0.060897821639186424\n",
      "Spanish e -> 0.12552653748946924\n",
      "Spanish c -> 0.05295462751233602\n",
      "Spanish l -> 0.05271392466000722\n",
      "Spanish a -> 0.10903839210494644\n",
      "Spanish r -> 0.06462871585028282\n",
      "Spanish i -> 0.07678420989288723\n",
      "Spanish รณ -> 0.009267059814658803\n",
      "Spanish n -> 0.07245155855096883\n",
      "Spanish u -> 0.03393910217836081\n"
     ]
    }
   ],
   "source": [
    "language_model_cfd = build_language_models(languages, language_base)\n",
    "print(language_model_cfd.conditions())\n",
    "# print the models for visual inspection (you always should have a look at the data :)\n",
    "for language in languages:\n",
    "    for key in list(language_model_cfd[language].keys())[:10]:\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rCCWTozF-E_"
   },
   "source": [
    "b) Develop an algorithm which calculates the overall score of a given text based on the frequency of characters\n",
    "accessible by `language_model_cfd[language].freq(character)`. Explain how the algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "tONTEw5nF-Sz"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-8fe79b118793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlanguage_model_cfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "prob = 0\n",
    "for char in text.lower():\n",
    "    prob += language_model_cfd[language].freq(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpscJYRTt_x0"
   },
   "source": [
    "The algorithm simply interates over all characters in the given text and sums up the frequencies of the individual characters given in the freqDist. To avoid a case sensitive algorithm, the text is converted into all lowercase by using text.lower()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVCvz3mnGhwC"
   },
   "source": [
    "c) Implement a function `guess_language(language_model_cfd,text) `that returns the most likely language\n",
    "for a given text according to your algorithm from the previous sub task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "crSfb-deGh-l"
   },
   "outputs": [],
   "source": [
    "def guess_language(language_model_cfd, text):\n",
    "    probabilites = dict()\n",
    "    for language in language_model_cfd.conditions():\n",
    "        prob = 0\n",
    "        for char in text.lower():\n",
    "            prob += language_model_cfd[language].freq(char)\n",
    "        probabilites[language] = prob\n",
    "        \n",
    "    return max(probabilites, key=probabilites.get), probabilites[max(probabilites, key=probabilites.get)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02-goJWNGrWC"
   },
   "source": [
    "d) Test your implementation with the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "aHNGzdWGGri8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter had been to the office before they arrived.\n",
      "peter had been to the office before they arrived.\n",
      "peter had been to the office before they arrived.\n",
      "guess for english text is ('German_Deutsch', 3.0331543310763487)\n",
      "si terminas tu tarea, te dare un caramelo.\n",
      "si terminas tu tarea, te dare un caramelo.\n",
      "si terminas tu tarea, te dare un caramelo.\n",
      "guess for spanish text is ('Spanish', 2.5370080635455525)\n",
      "das ist ein schon recht langes deutsches beispiel.\n",
      "das ist ein schon recht langes deutsches beispiel.\n",
      "das ist ein schon recht langes deutsches beispiel.\n",
      "guess for german text is ('German_Deutsch', 3.0392248424001864)\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Peter had been to the office before they arrived.\"\n",
    "text2 = \"Si terminas tu tarea, te dare un caramelo.\"\n",
    "text3 = \"Das ist ein schon recht langes deutsches Beispiel.\"\n",
    "\n",
    "# guess the language by comparing the frequency distributions\n",
    "print('guess for english text is', guess_language(language_model_cfd, text1))\n",
    "print('guess for spanish text is', guess_language(language_model_cfd, text2))\n",
    "print('guess for german text is', guess_language(language_model_cfd, text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFWdJBrf7IcW"
   },
   "source": [
    "e) Discuss, why English and German texts are difficult to distinguish with the given approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEzjSq_THwGc"
   },
   "source": [
    "This issue is most likely due to the fact that both English and German are Germanic langauges. Therefore, they have a similar vocabulary and also a more similar character distribution. This is clearly not the case with Spanish, as Spanish is a Romance langauge meaning that there are less similarities between German/English and Spanish than between German and English. We would most likely run into the same problem if we were to test this approach on Spanish and Portuguese or Spanish and Italian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwawqiMx77Yl"
   },
   "source": [
    "## Homework 4.2 (4 points)\n",
    "\n",
    "The previous language guesser was based on the frequency of characters. Implement alternative language guesser\n",
    "based on the following lexical units:\n",
    "\n",
    "In all of the following exercises, large parts of the code were copied from Homework 4.2. For each exercise, the only thing that had to be changed were the events when creating the ConditionalFreqDist and the algorthim described in 4.1.b.\n",
    "\n",
    "a) tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "FrLvV_Ra74Tp"
   },
   "outputs": [],
   "source": [
    "def build_language_models(languages, words):\n",
    "    freqDist = nltk.ConditionalFreqDist((language, word.lower()) for language in languages for word in words[language])  \n",
    "    return freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "CGizC_FxJg1m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English universal -> 0.002807411566535654\n",
      "English declaration -> 0.003368893879842785\n",
      "English of -> 0.045480067377877596\n",
      "English human -> 0.0072992700729927005\n",
      "English rights -> 0.010106681639528355\n",
      "English preamble -> 0.0005614823133071309\n",
      "English whereas -> 0.0039303761931499155\n",
      "English recognition -> 0.0016844469399213925\n",
      "English the -> 0.06176305446378439\n",
      "English inherent -> 0.0005614823133071309\n",
      "German_Deutsch die -> 0.026298487836949377\n",
      "German_Deutsch allgemeine -> 0.003287310979618672\n",
      "German_Deutsch erklรคrung -> 0.003287310979618672\n",
      "German_Deutsch der -> 0.03353057199211045\n",
      "German_Deutsch menschenrechte -> 0.0039447731755424065\n",
      "German_Deutsch resolution -> 0.0006574621959237344\n",
      "German_Deutsch 217 -> 0.0006574621959237344\n",
      "German_Deutsch a -> 0.0006574621959237344\n",
      "German_Deutsch ( -> 0.0006574621959237344\n",
      "German_Deutsch iii -> 0.0006574621959237344\n",
      "Spanish declaraciรณn -> 0.0022688598979013048\n",
      "Spanish universal -> 0.0022688598979013048\n",
      "Spanish de -> 0.06466250709018719\n",
      "Spanish derechos -> 0.009075439591605219\n",
      "Spanish humanos -> 0.003403289846851957\n",
      "Spanish adoptada -> 0.0005672149744753262\n",
      "Spanish y -> 0.04254112308564946\n",
      "Spanish proclamada -> 0.0005672149744753262\n",
      "Spanish por -> 0.011344299489506523\n",
      "Spanish la -> 0.046511627906976744\n"
     ]
    }
   ],
   "source": [
    "language_model_cfd = build_language_models(languages, language_base)\n",
    "\n",
    "# print the models for visual inspection (you always should have a look at the data :)\n",
    "for language in languages:\n",
    "    for key in list(language_model_cfd[language].keys())[:10]:\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "GM6crcvzJXVF"
   },
   "outputs": [],
   "source": [
    "def guess_language(language_model_cfd, text):\n",
    "    probabilites = dict()\n",
    "    for language in language_model_cfd.conditions():\n",
    "        prob = 0\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            prob += language_model_cfd[language].freq(token.lower())\n",
    "        probabilites[language] = prob\n",
    "    return max(probabilites, key=probabilites.get), probabilites[max(probabilites, key=probabilites.get)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "UNBB9OuLJY_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess for english text is ('English', 0.1420550252667041)\n",
      "guess for spanish text is ('German_Deutsch', 0.09993425378040763)\n",
      "guess for german text is ('German_Deutsch', 0.08086785009861933)\n"
     ]
    }
   ],
   "source": [
    "# guess the language by comparing the frequency distributions\n",
    "print('guess for english text is', guess_language(language_model_cfd, text1))\n",
    "print('guess for spanish text is', guess_language(language_model_cfd, text2))\n",
    "print('guess for german text is', guess_language(language_model_cfd, text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WdD9nLk8IIy"
   },
   "source": [
    "b) character bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "xqVsZQLtJrkO"
   },
   "outputs": [],
   "source": [
    "def build_language_models(languages, words):\n",
    "    freqDist = nltk.ConditionalFreqDist((language, bigram) for language in languages for word in words[language] for bigram in nltk.bigrams(word.lower()))  \n",
    "    return freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "J3DH5uK4JrkO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ('u', 'n') -> 0.0063174114021571645\n",
      "English ('n', 'i') -> 0.004930662557781202\n",
      "English ('i', 'v') -> 0.0032357473035439137\n",
      "English ('v', 'e') -> 0.01140215716486903\n",
      "English ('e', 'r') -> 0.020338983050847456\n",
      "English ('r', 's') -> 0.003389830508474576\n",
      "English ('s', 'a') -> 0.001694915254237288\n",
      "English ('a', 'l') -> 0.019106317411402157\n",
      "English ('d', 'e') -> 0.00600924499229584\n",
      "English ('e', 'c') -> 0.007087827426810477\n",
      "German_Deutsch ('d', 'i') -> 0.0099361249112846\n",
      "German_Deutsch ('i', 'e') -> 0.016465578424414477\n",
      "German_Deutsch ('a', 'l') -> 0.008232789212207239\n",
      "German_Deutsch ('l', 'l') -> 0.005961674946770759\n",
      "German_Deutsch ('l', 'g') -> 0.00127750177430802\n",
      "German_Deutsch ('g', 'e') -> 0.02185947480482612\n",
      "German_Deutsch ('e', 'm') -> 0.0055358410220014195\n",
      "German_Deutsch ('m', 'e') -> 0.006813342796309439\n",
      "German_Deutsch ('e', 'i') -> 0.02995031937544358\n",
      "German_Deutsch ('i', 'n') -> 0.01973030518097942\n",
      "Spanish ('d', 'e') -> 0.03513596089214788\n",
      "Spanish ('e', 'c') -> 0.01435991445157348\n",
      "Spanish ('c', 'l') -> 0.00229147571035747\n",
      "Spanish ('l', 'a') -> 0.01665139016193095\n",
      "Spanish ('a', 'r') -> 0.013443324167430493\n",
      "Spanish ('r', 'a') -> 0.012526733883287504\n",
      "Spanish ('a', 'c') -> 0.010388023220287198\n",
      "Spanish ('c', 'i') -> 0.021998166819431713\n",
      "Spanish ('i', 'รณ') -> 0.010846318362358692\n",
      "Spanish ('รณ', 'n') -> 0.010846318362358692\n"
     ]
    }
   ],
   "source": [
    "language_model_cfd = build_language_models(languages, language_base)\n",
    "\n",
    "# print the models for visual inspection (you always should have a look at the data :)\n",
    "for language in languages:\n",
    "    for key in list(language_model_cfd[language].keys())[:10]:\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "XWWnUeDMJrkO"
   },
   "outputs": [],
   "source": [
    "def guess_language(language_model_cfd, text):\n",
    "    probabilites = dict()\n",
    "    for language in language_model_cfd.conditions():\n",
    "        prob = 0\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            for bigram in nltk.bigrams(token.lower()):\n",
    "                prob += language_model_cfd[language].freq(bigram)\n",
    "        probabilites[language] = prob\n",
    "    return max(probabilites, key=probabilites.get), probabilites[max(probabilites, key=probabilites.get)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "rz-Bk5fzJrkO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess for english text is ('English', 0.3320493066255778)\n",
      "guess for spanish text is ('Spanish', 0.26336694164375196)\n",
      "guess for german text is ('German_Deutsch', 0.4481192334989355)\n"
     ]
    }
   ],
   "source": [
    "# guess the language by comparing the frequency distributions\n",
    "print('guess for english text is', guess_language(language_model_cfd, text1))\n",
    "print('guess for spanish text is', guess_language(language_model_cfd, text2))\n",
    "print('guess for german text is', guess_language(language_model_cfd, text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFo8i_SG8PGV"
   },
   "source": [
    "c) token bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "XpPnGvxEJ_Tt"
   },
   "outputs": [],
   "source": [
    "def build_language_models(languages, words):\n",
    "    freqDist = nltk.ConditionalFreqDist((language, bigram) for language in languages for bigram in nltk.bigrams(words[language]))  \n",
    "    return freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "UO8R67ApJ_Tt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ('Universal', 'Declaration') -> 0.0011235955056179776\n",
      "English ('Declaration', 'of') -> 0.0011235955056179776\n",
      "English ('of', 'Human') -> 0.0011235955056179776\n",
      "English ('Human', 'Rights') -> 0.0011235955056179776\n",
      "English ('Rights', 'Preamble') -> 0.0005617977528089888\n",
      "English ('Preamble', 'Whereas') -> 0.0005617977528089888\n",
      "English ('Whereas', 'recognition') -> 0.0005617977528089888\n",
      "English ('recognition', 'of') -> 0.0005617977528089888\n",
      "English ('of', 'the') -> 0.011235955056179775\n",
      "English ('the', 'inherent') -> 0.0005617977528089888\n",
      "German_Deutsch ('Die', 'Allgemeine') -> 0.0006578947368421052\n",
      "German_Deutsch ('Allgemeine', 'Erklรคrung') -> 0.0013157894736842105\n",
      "German_Deutsch ('Erklรคrung', 'der') -> 0.0013157894736842105\n",
      "German_Deutsch ('der', 'Menschenrechte') -> 0.002631578947368421\n",
      "German_Deutsch ('Menschenrechte', 'Resolution') -> 0.0006578947368421052\n",
      "German_Deutsch ('Resolution', '217') -> 0.0006578947368421052\n",
      "German_Deutsch ('217', 'A') -> 0.0006578947368421052\n",
      "German_Deutsch ('A', '(') -> 0.0006578947368421052\n",
      "German_Deutsch ('(', 'III') -> 0.0006578947368421052\n",
      "German_Deutsch ('III', ')') -> 0.0006578947368421052\n",
      "Spanish ('Declaraciรณn', 'Universal') -> 0.0011350737797956867\n",
      "Spanish ('Universal', 'de') -> 0.0011350737797956867\n",
      "Spanish ('de', 'Derechos') -> 0.0011350737797956867\n",
      "Spanish ('Derechos', 'Humanos') -> 0.0011350737797956867\n",
      "Spanish ('Humanos', 'Adoptada') -> 0.0005675368898978433\n",
      "Spanish ('Adoptada', 'y') -> 0.0005675368898978433\n",
      "Spanish ('y', 'proclamada') -> 0.0005675368898978433\n",
      "Spanish ('proclamada', 'por') -> 0.0005675368898978433\n",
      "Spanish ('por', 'la') -> 0.0022701475595913734\n",
      "Spanish ('la', 'Asamblea') -> 0.0005675368898978433\n"
     ]
    }
   ],
   "source": [
    "language_model_cfd = build_language_models(languages, language_base)\n",
    "\n",
    "# print the models for visual inspection (you always should have a look at the data :)\n",
    "for language in languages:\n",
    "    for key in list(language_model_cfd[language].keys())[:10]:\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "lTDVkP68J_Tt"
   },
   "outputs": [],
   "source": [
    "def guess_language(language_model_cfd, text):\n",
    "    probabilites = dict()\n",
    "    for language in language_model_cfd.conditions():\n",
    "        prob = 0\n",
    "        for bigram in nltk.bigrams(text):\n",
    "            prob += language_model_cfd[language].freq(bigram)\n",
    "        probabilites[language] = prob\n",
    "    print(probabilites)\n",
    "    return max(probabilites, key=probabilites.get), probabilites[max(probabilites, key=probabilites.get)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "1mF28SghJ_Tt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 0.0, 'German_Deutsch': 0.0, 'Spanish': 0.0}\n",
      "guess for english text is ('English', 0.0)\n",
      "{'English': 0.0, 'German_Deutsch': 0.0, 'Spanish': 0.0}\n",
      "guess for spanish text is ('English', 0.0)\n",
      "{'English': 0.0, 'German_Deutsch': 0.0, 'Spanish': 0.0}\n",
      "guess for german text is ('English', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# guess the language by comparing the frequency distributions\n",
    "print('guess for english text is', guess_language(language_model_cfd, text1))\n",
    "print('guess for spanish text is', guess_language(language_model_cfd, text2))\n",
    "print('guess for german text is', guess_language(language_model_cfd, text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfDmJsx08g7N"
   },
   "source": [
    "d) Discuss, which approach should work best theoretically. Is this reflected in the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHhi6mysKG1Q"
   },
   "source": [
    "In theory approaches that use bigrams vs. single characters/tokens should work better as they take the context of the character/token into consideration. This is also reflected to a certain extent in the results. The approach using the character bigrams was the only one to correctly guess the language of all three texts. Nonetheless, the approach using token bigrams failed completely. The probabilty for each language in each of the three test cases was 0. The reason for this is most likely not due to the approach itself but rather to the fact that we only used one (very specific) text to train our model. The three test texts were:\n",
    "\n",
    "`\n",
    "text1 = \"Peter had been to the office before they arrived.\"\n",
    "text2 = \"Si terminas tu tarea, te dare un caramelo.\"\n",
    "text3 = \"Das ist ein schon recht langes deutsches Beispiel.\"\n",
    "`\n",
    "\n",
    "Just by looking at the texts, one can easily see that they have very little, if anything at all, in common with the Declaration of Human Rights. Therefore, it is very unlikely that we will find matching bigrams in our model. For this reason, we would need to train our model with more texts and also make sure that we don't exclusivly have text with a very specific domain. If this were done, the results for the token bigram approach would probably be a lot more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogZK3GpCKLKy"
   },
   "source": [
    "## Homework 4.3 \n",
    "*(This homework is not part of the bonus system. However, we recommend you to work it out. It will save you some time in the future.)*\n",
    "\n",
    "Copy all functions implemented in the tasks and homeworks to one file and name it `UKP_Lib.py`. You may easily access for examle the function `word_freq` of the previous tasks with the following statement:\n",
    "\n",
    "`from UKP_Lib import word_freq`\n",
    "\n",
    "You just implemented your first module. If you are familiar with another object oriented language, feel free to use classes and OO in the exercises. Make yourself familiar with syntax of OO-constructs in Python, e.g. consult http://docs.python.org/tutorial/classes.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework4Updated.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
