{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FoLT Homework 3\n",
    "## Alexander Praus, Maike Arnold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up\n",
    "from nltk import *\n",
    "from nltk.book import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the whole corpus as a list of words\n",
    "# corpus_words = nltk.corpus.brown.words()\n",
    "\n",
    "# Access ids of texts that are available in the corpus:\n",
    "# print(\"File ids: \", nltk.corpus.brown.fileids())\n",
    "\n",
    "# Retrieve a certain text by id\n",
    "# file_words = nltk.corpus.brown.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step isn't really necessary. We use the mentioned functions in b) to get the list of words of the corpus/text and immediately create a FreqDist of the bigrams in the respective text/corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_fg = FreqDist(bigrams(corpus.gutenberg.words('carroll-alice.txt')))\n",
    "\n",
    "fdist_bg = FreqDist(bigrams(corpus.brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_LL(phrase, fdist_fg, fdist_bg):\n",
    "    A = fdist_fg[phrase]\n",
    "    B = fdist_bg[phrase]\n",
    "    C = fdist_fg.N()\n",
    "    D = fdist_bg.N()\n",
    "    E1 = C*(A+B)/(C+D)\n",
    "    E2 = D*(A+B)/(C+D)\n",
    "\n",
    "    T1 = 0\n",
    "    T2 = 0\n",
    "    \n",
    "    if A != 0 and E1 != 0:\n",
    "        T1 = A * math.log(A/E1)\n",
    "    if B != 0 and E2 != 0:\n",
    "        T2 = B * math.log(B/E2)\n",
    "    \n",
    "    return 2 * (T1 + T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_results = FreqDist()\n",
    "\n",
    "# for each bigram\n",
    "for phrase in fdist_fg:\n",
    "    # optional step: filter out all phrases containing tokens that aren't alphabetic\n",
    "    if phrase[0].isalpha() and phrase[1].isalpha(): \n",
    "        ll_results[phrase] = compute_LL(phrase, fdist_fg, fdist_bg)\n",
    "        \n",
    "# return 10 most common (= most imporbable) phrases\n",
    "print(ll_results.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to filter and only use phrases that contain only alphabetic tokens to get more meaningful results. Another idea would be to filter by length as this would also remove tokens containing simple punctuation an also stop words such as \"if\", \"of\", etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
