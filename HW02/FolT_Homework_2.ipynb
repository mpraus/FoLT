{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXTZ3sthLr87"
   },
   "source": [
    "# **FolT Homework 2**\n",
    "## Alexander Praus, Maike Arnold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlpPiaEuSTw8"
   },
   "outputs": [],
   "source": [
    "# Execute before and only in colab, if you use jupyter ignore this cell\n",
    "!pip install --target=$nb_path nltk==3.5\n",
    "!python3 -m nltk.downloader popular\n",
    "!python3 -m nltk.downloader book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6ckUIz6TPJn"
   },
   "outputs": [],
   "source": [
    "# Excecute before\n",
    "from nltk import *\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo3mepfqL17n"
   },
   "source": [
    "### Homework 2.1 (5 points)\n",
    "An important problem in computational linguistics is morphological analysis. This consists of breaking down a word\n",
    "into its component pieces, for example losses might be broken down as loss + es. In English, morphology is relatively\n",
    "simple and is mostly comprised of prefixes and suffixes. To get an idea of what suffixes are common in English (and\n",
    "thus could be morphemes), we can look at the frequencies of the last two characters of sufficiently long words. Write\n",
    "a function ***top_suffixes(words)*** that takes a sequence of words as an input and returns the ***20 most frequent twocharacter suffixes*** of the input words. We define a two-character suffix as the last two characters of any word of ***length\n",
    "5*** or more, thus your function may simply ignore any word shorter than five characters. Your function should use the\n",
    "***NLTK FreqDist*** class to count these suffixes. Hint: You may use the following code snippets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoLbzwl9rr3k"
   },
   "outputs": [],
   "source": [
    "blake_words = corpus.gutenberg.words('blake-poems.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJUV0DoQ3jub"
   },
   "outputs": [],
   "source": [
    "# write your top_suffixes function here\n",
    "def top_suffixes(words):\n",
    "    # create FreqDist with suffixes (word[-2:]) for all words with length > 4    \n",
    "    suffixes = FreqDist([word[-2:] for word in words if len(word) > 4])\n",
    "    \n",
    "    # this returns list with tuples (suffix, count)\n",
    "#     return suffixes.most_common(20)\n",
    "\n",
    "    # only returns suffix, which is the desired output according to moodle forum\n",
    "    return [word for word, word_count in suffixes.most_common(20)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhYkdeLKrqPM"
   },
   "outputs": [],
   "source": [
    "print(top_suffixes(blake_words)) # check your function\n",
    "\n",
    "# \"ed\" is a one of the most common english language suffixes \n",
    "# So this test should equal to true otherwise you need to look for mistakes in your implementation\n",
    "print(\"ed\" in top_suffixes(blake_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD6pAe38SvnV"
   },
   "source": [
    "### Hints \n",
    "*   FreqDist(???).most_common(20) #what argument expects FreqDist?\n",
    "*   word[-2:] #result?\n",
    "* for word in words:\n",
    "  if len(word) > 4: ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FolT_Homework_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
